"""
Delta Analysis Service - Innovation 1 (Phase 7: 10K Scale)

Reduces LLM costs by 90% by:
1. Computing cost deltas (changes) instead of full 30-day data
2. Sending only the top movers and significant changes to LLM
3. Merging incremental insights with cached analysis

Usage:
    delta_service = DeltaAnalysisService(cache)
    delta_data = await delta_service.compute_delta(tenant_id, new_costs, previous_costs)
    if delta_data.has_significant_changes:
        analysis = await analyzer.analyze(delta_data.as_llm_prompt(), tenant_id)
"""

import json
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from datetime import date
from uuid import UUID
import structlog

from app.shared.core.cache import CacheService, get_cache_service

logger = structlog.get_logger()

# Delta thresholds
SIGNIFICANT_CHANGE_PERCENT = 15.0  # 15% change is significant
TOP_MOVERS_COUNT = 10  # Top 10 cost movers to include
MINIMUM_COST_THRESHOLD = 1.0  # Ignore resources under $1/day


@dataclass
class CostDelta:
    """Represents cost change for a single resource."""
    resource_id: str
    resource_type: str
    previous_cost: float
    current_cost: float
    change_amount: float
    change_percent: float
    region: str = ""
    
    @property
    def is_significant(self) -> bool:
        """Check if change exceeds threshold."""
        return abs(self.change_percent) >= SIGNIFICANT_CHANGE_PERCENT
    
    @property
    def is_spike(self) -> bool:
        """Check if this is a cost spike (increase > 50%)."""
        return self.change_percent > 50.0
    
    @property
    def is_drop(self) -> bool:
        """Check if this is a cost drop (decrease > 30%)."""
        return self.change_percent < -30.0


@dataclass
class DeltaAnalysisResult:
    """Result of delta analysis with optimized data for LLM."""
    tenant_id: UUID
    analysis_date: date
    
    # Cost summaries
    total_previous: float = 0.0
    total_current: float = 0.0
    total_change: float = 0.0
    total_change_percent: float = 0.0
    
    # Significant changes
    top_increases: List[CostDelta] = field(default_factory=list)
    top_decreases: List[CostDelta] = field(default_factory=list)
    new_resources: List[Dict[str, Any]] = field(default_factory=list)
    removed_resources: List[Dict[str, Any]] = field(default_factory=list)
    
    # Analysis metadata
    days_compared: int = 3
    resources_analyzed: int = 0
    significant_changes_count: int = 0
    
    @property
    def has_significant_changes(self) -> bool:
        """Check if there are any significant changes worth analyzing."""
        return (
            len(self.top_increases) > 0 or
            len(self.top_decreases) > 0 or
            len(self.new_resources) > 0 or
            abs(self.total_change_percent) >= SIGNIFICANT_CHANGE_PERCENT
        )
    
    def as_llm_prompt_data(self) -> Dict[str, Any]:
        """
        Convert to optimized dict for LLM prompt.
        This is ~10x smaller than sending full 30-day cost data.
        """
        return {
            "analysis_type": "delta",
            "period": f"Last {self.days_compared} days",
            "summary": {
                "total_previous_cost": f"${self.total_previous:.2f}",
                "total_current_cost": f"${self.total_current:.2f}",
                "total_change": f"${self.total_change:+.2f}",
                "change_percent": f"{self.total_change_percent:+.1f}%",
                "resources_with_changes": self.significant_changes_count
            },
            "top_cost_increases": [
                {
                    "resource": d.resource_id,
                    "type": d.resource_type,
                    "change": f"${d.change_amount:+.2f}/day",
                    "percent": f"{d.change_percent:+.1f}%",
                    "current": f"${d.current_cost:.2f}/day"
                }
                for d in self.top_increases[:5]
            ],
            "top_cost_decreases": [
                {
                    "resource": d.resource_id,
                    "type": d.resource_type,
                    "change": f"${d.change_amount:+.2f}/day",
                    "percent": f"{d.change_percent:+.1f}%"
                }
                for d in self.top_decreases[:5]
            ],
            "new_resources": self.new_resources[:5],
            "instructions": (
                "Analyze these cost CHANGES (delta) from the past few days. "
                "Focus on: 1) Reasons for top increases, 2) New resources that appeared, "
                "3) Potential anomalies or waste. Provide actionable recommendations."
            )
        }
    
    def as_json(self) -> str:
        """Serialize for LLM consumption."""
        return json.dumps(self.as_llm_prompt_data(), indent=2)


class DeltaAnalysisService:
    """
    Service for computing cost deltas and optimizing LLM input.
    
    Reduces LLM API costs by ~90% through:
    - Sending 3-day deltas instead of 30-day full data
    - Including only top movers and significant changes
    - Filtering out noise (small resources, minimal changes)
    """
    
    def __init__(self, cache: Optional[CacheService] = None):
        self.cache = cache or get_cache_service()
    
    async def compute_delta(
        self,
        tenant_id: UUID,
        current_costs: List[Dict[str, Any]],
        previous_costs: Optional[List[Dict[str, Any]]] = None,
        days_to_compare: int = 3
    ) -> DeltaAnalysisResult:
        """
        Compute cost delta between current and previous periods.
        
        Args:
            tenant_id: Tenant identifier
            current_costs: Recent cost data (last few days)
            previous_costs: Optional previous period for comparison
            days_to_compare: Number of days to include in delta
        
        Returns:
            DeltaAnalysisResult with optimized data for LLM
        """
        logger.info(
            "computing_cost_delta",
            tenant_id=str(tenant_id),
            current_records=len(current_costs),
            previous_records=len(previous_costs) if previous_costs else 0
        )
        
        result = DeltaAnalysisResult(
            tenant_id=tenant_id,
            analysis_date=date.today(),
            days_compared=days_to_compare
        )
        
        # Parse cost data into resource-level aggregates
        current_by_resource = self._aggregate_by_resource(current_costs, days_to_compare)
        previous_by_resource = self._aggregate_by_resource(
            previous_costs, days_to_compare
        ) if previous_costs else {}
        
        result.resources_analyzed = len(current_by_resource)
        
        # Calculate totals
        result.total_current = sum(r["daily_cost"] for r in current_by_resource.values())
        result.total_previous = sum(r["daily_cost"] for r in previous_by_resource.values())
        result.total_change = result.total_current - result.total_previous
        if result.total_previous > 0:
            result.total_change_percent = (result.total_change / result.total_previous) * 100
        
        # Find deltas for each resource
        all_deltas: List[CostDelta] = []
        all_resource_ids = set(current_by_resource.keys()) | set(previous_by_resource.keys())
        
        for resource_id in all_resource_ids:
            current = current_by_resource.get(resource_id)
            previous = previous_by_resource.get(resource_id)
            
            # New resource
            if current and not previous:
                if current["daily_cost"] >= MINIMUM_COST_THRESHOLD:
                    result.new_resources.append({
                        "resource": resource_id,
                        "type": current.get("type", "Unknown"),
                        "cost": f"${current['daily_cost']:.2f}/day"
                    })
                continue
            
            # Removed resource
            if previous and not current:
                result.removed_resources.append({
                    "resource": resource_id,
                    "type": previous.get("type", "Unknown"),
                    "previous_cost": f"${previous['daily_cost']:.2f}/day"
                })
                continue
            
            # Calculate delta
            if current and previous:
                prev_cost = previous["daily_cost"]
                curr_cost = current["daily_cost"]
                change = curr_cost - prev_cost
                
                # Skip insignificant changes
                if abs(change) < 0.50:  # Less than $0.50/day change
                    continue
                
                change_pct = (change / prev_cost * 100) if prev_cost > 0 else 0
                
                delta = CostDelta(
                    resource_id=resource_id,
                    resource_type=current.get("type", "Unknown"),
                    previous_cost=prev_cost,
                    current_cost=curr_cost,
                    change_amount=change,
                    change_percent=change_pct,
                    region=current.get("region", "")
                )
                
                if delta.is_significant:
                    result.significant_changes_count += 1
                
                all_deltas.append(delta)
        
        # Sort and select top movers
        increases = [d for d in all_deltas if d.change_amount > 0]
        decreases = [d for d in all_deltas if d.change_amount < 0]
        
        result.top_increases = sorted(
            increases, 
            key=lambda x: x.change_amount, 
            reverse=True
        )[:TOP_MOVERS_COUNT]
        
        result.top_decreases = sorted(
            decreases, 
            key=lambda x: x.change_amount
        )[:TOP_MOVERS_COUNT]
        
        logger.info(
            "delta_computed",
            tenant_id=str(tenant_id),
            total_change=f"${result.total_change:+.2f}",
            significant_changes=result.significant_changes_count,
            has_changes=result.has_significant_changes
        )
        
        return result
    
    def _aggregate_by_resource(
        self, 
        costs: Optional[List[Dict[str, Any]]], 
        days: int
    ) -> Dict[str, Dict[str, Any]]:
        """
        Aggregate cost data by resource ID, computing daily averages.
        
        Handles AWS Cost Explorer format:
        {
            "TimePeriod": {"Start": "2026-01-01", "End": "2026-01-02"},
            "Groups": [{"Keys": ["service", "resource"], "Metrics": {...}}]
        }
        """
        if not costs:
            return {}
        
        resource_totals: Dict[str, Dict[str, Any]] = {}
        
        for day_data in costs[-days:]:  # Only last N days
            groups = day_data.get("Groups", [])
            
            for group in groups:
                keys = group.get("Keys", [])
                if len(keys) >= 1:
                    # Use first key as resource identifier
                    resource_id = keys[-1] if len(keys) > 1 else keys[0]
                    resource_type = keys[0] if len(keys) > 1 else "Service"
                    
                    metrics = group.get("Metrics", {})
                    amount = float(
                        metrics.get("UnblendedCost", {}).get("Amount", 0) or
                        metrics.get("BlendedCost", {}).get("Amount", 0) or 0
                    )
                    
                    if resource_id not in resource_totals:
                        resource_totals[resource_id] = {
                            "type": resource_type,
                            "total_cost": 0.0,
                            "days": 0,
                            "region": ""
                        }
                    
                    resource_totals[resource_id]["total_cost"] += amount
                    resource_totals[resource_id]["days"] += 1
        
        # Calculate daily averages
        for resource_id, data in resource_totals.items():
            days_counted = data["days"] or 1
            data["daily_cost"] = data["total_cost"] / days_counted
        
        return resource_totals


async def analyze_with_delta(
    analyzer,
    tenant_id: UUID,
    current_costs: List[Dict[str, Any]],
    previous_costs: Optional[List[Dict[str, Any]]] = None,
    db = None,
    force_refresh: bool = False
) -> str:
    """
    Convenience function to perform delta-optimized analysis.
    
    1. Computes cost delta (3-day changes)
    2. If significant changes exist, runs LLM analysis on delta
    3. Merges with cached results if available
    4. Falls back to full analysis if no previous data
    
    Returns:
        JSON string with analysis results
    """
    cache = get_cache_service()
    delta_service = DeltaAnalysisService(cache)
    
    # Check for cached analysis first
    if not force_refresh:
        cached = await cache.get_analysis(tenant_id)
        if cached:
            logger.info("delta_analysis_cache_hit", tenant_id=str(tenant_id))
            return json.dumps(cached)
    
    # Compute delta
    delta = await delta_service.compute_delta(
        tenant_id=tenant_id,
        current_costs=current_costs,
        previous_costs=previous_costs
    )
    
    # If no significant changes, return a simple response
    if not delta.has_significant_changes:
        logger.info(
            "delta_no_significant_changes",
            tenant_id=str(tenant_id)
        )
        result = {
            "status": "no_significant_changes",
            "summary": {
                "message": f"No significant cost changes in the last {delta.days_compared} days",
                "total_change": f"${delta.total_change:+.2f}",
                "change_percent": f"{delta.total_change_percent:+.1f}%"
            },
            "anomalies": [],
            "zombie_resources": [],
            "recommendations": []
        }
        await cache.set_analysis(tenant_id, result)
        return json.dumps(result)
    
    # Run LLM analysis on delta data only
    logger.info(
        "delta_analysis_running_llm",
        tenant_id=str(tenant_id),
        significant_changes=delta.significant_changes_count
    )
    
    # Pass the optimized delta data to analyzer
    delta_prompt_data = delta.as_llm_prompt_data()
    
    # Use the analyzer with delta data
    result = await analyzer.analyze(
        cost_data=[delta_prompt_data],  # Much smaller than 30-day data
        tenant_id=tenant_id,
        db=db,
        force_refresh=True  # We're passing processed delta, not raw data
    )
    
    return result
